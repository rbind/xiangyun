---
title: "scikit-learn 做机器学习"
author: "黄湘云"
date: '2025-11-01'
slug: scikit-learn
categories:
  - 统计软件
tags:
  - scikit-learn
  - 机器学习
  - 分类问题
  - NumPy
  - SciPy
  - matrix
  - array
output:
  blogdown::html_page:
    toc: true
    number_sections: true
link-citations: true
description: "Python 模块 scikit-learn 是做机器学习的，本文添加等价 R 代码。"
---

本文基于 iris 数据集介绍 scikit-learn (<https://scikit-learn.org/stable/>) 的简单用法，并提供 R 语言的等价表示。

scikit-learn 整合各类机器学习算法，以及整套流程（数据归一化、数据集切分、超参数寻优等），提供统一的接口，学习和使用上更加方便。
xgboost 的 R 包接口更接近 Python 编程的使用习惯。
想要对齐 Python 语言和 R 语言的结果是比较困难的，它们内部的实现方式不一致，只能在结果上是做到十分接近。

# 准备

导入必要的功能模块，加载 iris 数据集。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
import numpy as np
# 数据集 iris
from sklearn.datasets import load_iris
# 分类模型
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# 模型评估
from sklearn.metrics import accuracy_score, log_loss, roc_auc_score
# 加载 iris 数据
X, y = load_iris(return_X_y=True)
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r message=FALSE}
library(nnet) # 多项对数线性分类模型
library(glmnet) # 允许带惩罚项
library(randomForest) # 随机森林
library(xgboost)      # 梯度提升
# pROC::aoc()         # 计算 AUC
```

:::
::::::


# 分类模型


## 不带惩罚项

R 内置的 `glm()` 函数不支持三分类及以上的多类分类模型，而 nnet 包支持。其拟合函数 `multinom()` 内部调用优化函数 `optim()` ，优化求解器是 `BFGS` 。拟合结果中，第一个类的回归系数都是 0 。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

函数 `LogisticRegression()` 设置 `penalty=None` 表示对回归系数不加惩罚，参数估计要求解一个优化问题，`solver='lbfgs'` 设置优化求解器，`C=1` 默认值，设置正则项的系数，即惩罚强度，越小强度越大，`C=1` 表示不加强度。

```{python}
clf = LogisticRegression(penalty=None, random_state=0, solver='lbfgs', C=1).fit(X, y)
np.hstack((clf.intercept_.reshape(3, 1), clf.coef_))
# R 对数似然 = -150 * Python 对数损失
log_loss(y, clf.predict_proba(X))
-150 * log_loss(y, clf.predict_proba(X))
# 150 个样本预测正确的有 148 个，返回预测的准确度 148/150
clf.score(X, y)
# 与 clf.score 一样
accuracy_score(y, clf.predict(X))
# 计算 AUC
roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r}
library(nnet) # 多项逻辑回归
iris_nnet <- multinom(Species ~ ., data = iris, trace = FALSE)
summary(iris_nnet)
# 对数似然
logLik(iris_nnet)
# 预测，返回概率值
iris_nnet_pred <- predict(iris_nnet, iris[, -5], type = "prob")
# 混淆矩阵
table(iris$Species, predict(iris_nnet, iris, type = "class"))
# 计算 AUC
pROC::auc(pROC::multiclass.roc(iris[, 5], iris_nnet_pred))
```

:::
::::::


## 带 L2 （岭）惩罚

在函数 `LogisticRegression()` 中设置 `penalty="l2"` 表示添加 L2 （岭）惩罚。在 R 语言中，带惩罚项的回归模型，常用 **glmnet** 包来拟合。函数 `glmnet()` 设置 `alpha = 0` 表示添加 L2 （岭）惩罚，设置参数 `standardize = FALSE` 表示在拟合模型前数据不做标准化，这和下面的 Python 代码对齐。Python 函数 `LogisticRegression()` 中的参数 `C=1` 的作用相当于 R 函数 `glmnet()` 中 `penalty.factor = 1` 。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

当设置 `penalty="l2"` 时，默认的迭代次数（max_iter=100）不够用了，最大迭代次数设置为 1000 。

```{python}
clf = LogisticRegression(
  penalty="l2", random_state=0, 
  solver='lbfgs', C=1, 
  max_iter=1000).fit(X, y)
# 截距和系数
np.hstack((clf.intercept_.reshape(3, 1), clf.coef_))
# 预测的准确度
clf.score(X, y)
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r}
iris_glmnet <- glmnet(
  x = iris[, -5], y = iris[, 5],
  alpha = 0, penalty.factor = rep(1, 4),
  standardize = FALSE, family = "multinomial"
)
# 截距和系数
do.call("rbind", lapply(coef(iris_glmnet, s = iris_glmnet$lambda[60]), function(x) t(x)))
# 预测
iris_pred_glmnet <- predict(iris_glmnet, newx = as.matrix(iris[, -5]), s = iris_glmnet$lambda[60], type = "class")
# 混淆矩阵
table(iris_pred_glmnet, iris$Species)
```

:::
::::::


## 超参数寻优

常用交叉验证 CV 来调模型中超参数

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
# CV 选超参数 C
clf = LogisticRegressionCV(
  penalty="l2", random_state=0, 
  solver='lbfgs',
  max_iter=1000).fit(X, y)
# 截距和系数
np.hstack((clf.intercept_.reshape(3, 1), clf.coef_))
# 预测的准确度
clf.score(X, y)
```

CV 选更好的 C 之后，准确度提升了。
:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r}
# CV 选超参数 lambda
iris_cv_glmnet <- cv.glmnet(
  x = as.matrix(iris[, -5]),
  y = iris[, 5], alpha = 0, penalty.factor = rep(1, 4),
  standardize = FALSE, nlambda = 500,
  family = "multinomial"
)
# 最佳的 lambda
best_lambda <- iris_cv_glmnet$lambda.min
best_lambda
# 最佳的 lambda 代入模型
iris_glmnet2 <- glmnet(
  x = iris[, -5], y = iris[, 5],
  lambda = best_lambda,
  alpha = 0,
  standardize = FALSE, family = "multinomial"
)
# 最佳模型的回归系数
do.call("rbind", lapply(coef(iris_glmnet2), function(x) t(x)))
# 预测
iris_pred_glmnet2 <- predict(iris_glmnet2, newx = as.matrix(iris[, -5]), type = "class")
# 最佳模型的预测结果
table(iris_pred_glmnet2, iris$Species)
```

预测准确度 1-6/150 = 0.96 。
:::
::::::

## 随机森林 {#Random-Forest}

数据集比较简单，随机森林和梯度提升算法很容易过拟合的。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
clf = RandomForestClassifier(max_depth=2, random_state=0).fit(X, y)
clf.score(X, y)
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r}
library(randomForest) # 随机森林
iris_rf <- randomForest(
  Species ~ ., data = iris,
  importance = TRUE, proximity = TRUE
)
# 分类结果
print(iris_rf)
```

预测准确度 1-7/150 = 0.9533 。

:::
::::::


## 梯度提升 {#Gradient-Boosting}

梯度提升算法很容易过拟合的。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X, y)
clf.score(X, y)
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{r}
library(xgboost)
iris_xgb <- xgboost(
  x = iris[, -5], y = iris[, 5],
  objective = "multi:softprob", 
  max_depth = 2, # 树的深度
  learning_rate = 0.3 # 默认值
)
# 预测
iris_pred_xgb <- predict(object = iris_xgb, newdata = iris[, -5])
# 预测结果
head(iris_pred_xgb)
```

每一个样本划分到各个类的概率。如果以 0.9 为分割点，则概率大于 0.9 的视为样本归属于该类。

```{r}
apply(iris_pred_xgb, 2, cut, breaks = c(0, 0.9, 1)) |> 
  xtabs(formula = ~. )
```

属于 virginica 鸢尾的有 46 个，属于 setosa 鸢尾的有 50 个，属于 versicolor 鸢尾的有 47 个，还有 7 个样本归类错误。

```{r}
# 预测结果是类别，而不是概率分布
iris_pred = predict(iris_xgb, iris[, -5], type = "class")
table(iris$Species, iris_pred)
```

实际上，XGBoost 可以选择最佳的分割点 0.8，使得分类完全符合实际数据。

注意：函数 `xgboost()` 目前不支持设置 `objective = "multi:softmax"` 。函数 `xgb.train()` 支持，示例见《现代应用统计》分类问题中的 [集成学习](https://bookdown.org/xiangyun/masr/classification-problems.html#sec-random-forests)。

:::
::::::


# 运行环境

```{r}
xfun::session_info(packages = c(
  "nnet", "glmnet", "randomForest", "xgboost", "pROC"))
reticulate::py_config()
```

# 参考文献

1. sklearn 文档 [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
