---
title: Tensorflow 的张量（多维数组）操作
date: "2025-10-31"
author: "黄湘云"
slug: tensorflow
categories:
  - 统计软件
tags:
  - 深度学习框架
  - tensorflow
  - NumPy
  - 多维数组
  - 数值优化
output:
  blogdown::html_page:
    toc: true
    number_sections: true
link-citations: true
description: "本文介绍 Tensorflow 的张量（多维数组）操作。"
---



# 准备

创建数组

```{python}
import tensorflow as tf
A = tf.constant([[1.0, -1.0], [1.0, 1.0]])
A
import numpy as np
a = np.array([[1., -1.], [1., 1.]])
a
```

# 张量操作

## 基础操作

均值和矩阵乘法

```{python}
# 整个矩阵求平均值
tf.reduce_mean(A)
# 按列求平均值
tf.reduce_mean(A, axis = 0)
# 按行求平均值
tf.reduce_mean(A, axis = 1)

w = tf.Variable([[1.], [2.]]) # 2x1
x = tf.constant([[3., 4.]])   # 1x2
tf.matmul(w, x) # 矩阵乘法
```


## 线性代数

### SVD 分解

注意 NumPy 和 Tensorflow 分解矩阵的结果在表示上的差别。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
s, u, v = np.linalg.svd(a)
s
u
v

np.dot(s * u, v)
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
S, U, V = tf.linalg.svd(A)
S
U
V
tf.matmul(U, tf.matmul(tf.linalg.diag(S), V, adjoint_b=True))
```


:::
::::::

### QR 分解

- a is a tensor.
- q is a tensor of orthonormal matrices.
- r is a tensor of upper triangular matrices.


:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
q, r = np.linalg.qr(a)
q
r

q @ r
q @ q.T
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
Q, R = tf.linalg.qr(A)
Q
R
Q @ R # 等价于 tf.matmul(Q, R)
```

:::
::::::


# 数值优化

2017 年有篇介绍 tensorflow 的文章，挺有意思，拟合混合正态分布的参数，请看
[为什么统计学家也应该学学 TensorFlow](https://cosx.org/2017/08/tensorflow-for-statisticians/)。我这里简单点，直接拉来一个目标函数，复用 [SciPy 做科学计算](/2025/10/scipy/) 中的无约束非线性优化示例。

:::::: {.columns}
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
import numpy as np
from scipy.optimize import minimize

def rosen(x):
    """The Rosenbrock function"""
    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
# 初始值
x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
# 调用优化器
res = minimize(rosen, x0, method='L-BFGS-B')
# 最终结果：最优解
print(res.x) 
```

:::
::: {.column width="5%" data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
::: {.column width="47.5%" data-latex="{0.475\textwidth}"}

```{python}
# 定义 Rosenbrock 函数（TensorFlow 版本）
# tf.reduce_sum tf.square 类似 tf.reduce_mean 是基础操作函数
def rosen_tf(x):
    return tf.reduce_sum(
        100.0 * tf.square(x[1:] - tf.square(x[:-1])) + tf.square(1.0 - x[:-1])
    )

# 初始化变量（与原始 SciPy 代码相同的初始点）
x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2], dtype=np.float32)
# x 是可训练的变量
x = tf.Variable(x0, trainable=True, dtype=tf.float32)

# 创建优化器（使用 Adam 优化器，学习率可根据需要调整）
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

# 优化过程：自动微分、梯度优化
# 迭代 1000 次会更加接近最优解
for _ in range(10):
    with tf.GradientTape() as tape:
        # 损失函数
        loss = rosen_tf(x)
    # 自动微分
    gradients = tape.gradient(loss, [x])
    # 梯度优化
    optimizer.apply_gradients(zip(gradients, [x]))
# 最优解
x.numpy()
# 目标函数值
loss.numpy()
loss
# 目标函数的梯度
gradients
```

:::
::::::

类似 SciPy 的优化求解器来自 TensorFlow Probability，如下代码调整自 API 文档[tfp.optimizer.lbfgs_minimize](https://tensorflow.google.cn/probability/api_docs/python/tfp/optimizer/lbfgs_minimize)。

```bash
pip install tensorflow_probability tf-keras
```

```{python}
import tensorflow_probability as tfp

# 目标函数及梯度
def rosen_loss_and_gradient(x):
    return tfp.math.value_and_gradient(rosen_tf, x)
# L-BFGS 优化器
optim_results = tfp.optimizer.lbfgs_minimize(
    value_and_gradients_function=rosen_loss_and_gradient,
    initial_position=x0,
    num_correction_pairs=10,
    tolerance=1e-8
  )

# 迭代次数
optim_results.num_iterations.numpy()
# 结果显示收敛
optim_results.converged.numpy()
# 最优解
optim_results.position
optim_results.position.numpy()
# 目标函数值
optim_results.objective_value.numpy()
# 目标函数计算次数
optim_results.num_objective_evaluations.numpy()
```


走马观花地翻了下 Tensorflow 的 API 目录，产生 Tensorflow 在手，天下我有的感觉。

